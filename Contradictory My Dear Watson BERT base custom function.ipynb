{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"155bu4BBCGpQEs54PCqreVtdUwikmA3NU","authorship_tag":"ABX9TyOTwDyNHf2TOk30M6QTfLQP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Rn01Bh5KFS6","executionInfo":{"status":"ok","timestamp":1680109351520,"user_tz":240,"elapsed":14546,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}},"outputId":"b6cb0573-def6-4bd4-afbb-f43076c7f8fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.3\n"]}],"source":["pip install transformers"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import random\n","from tqdm.notebook import tqdm\n","import transformers\n","import torch\n","import torch.nn as nn\n","import random\n","import os\n","\n","from torch.nn import functional as F\n","from transformers import RobertaTokenizer, BertTokenizer\n","from torch.utils.data import Dataset, DataLoader\n","# from transformers import BertForSequenceClassification \n","from transformers import AdamW\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support"],"metadata":{"id":"86VB4nOnKP0Z","executionInfo":{"status":"ok","timestamp":1680109496651,"user_tz":240,"elapsed":7465,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 32\n","MODEL_MAX_LENGTH = 256"],"metadata":{"id":"C98ODWcfD743","executionInfo":{"status":"ok","timestamp":1680109496652,"user_tz":240,"elapsed":7,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def loss_fn(output, target):\n","    return nn.CrossEntropyLoss()(output, target)"],"metadata":{"id":"bqL9jEdQKPpx","executionInfo":{"status":"ok","timestamp":1680109497560,"user_tz":240,"elapsed":913,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class MyDataSet(Dataset):\n","  def __init__(self, **kwargs):\n","    super().__init__()\n","    pass\n","    \n","  def __len__(self):\n","    pass \n","\n","  def __getitem__(self):\n","    pass"],"metadata":{"id":"_hFpnJLdKPnQ","executionInfo":{"status":"ok","timestamp":1680109497560,"user_tz":240,"elapsed":2,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class MyDataSet(Dataset):\n","  def __init__(self, df, tokenizer, max_length, tk_with = \"bert\"):\n","    super().__init__()\n","    self.df = df\n","    self.tokenizer = tokenizer\n","    self.max_length = max_length\n","    self.target = self.df.loc[:,'label']\n","    self.tk_with = tk_with\n","    \n","  def __len__(self):\n","    return self.df.shape[0]\n","\n","  def __getitem__(self, index):\n","    text1, text2 = self.df.iloc[index][['premise', 'hypothesis']]\n","\n","    if self.tk_with == \"bert\" :\n","      inputs = self.tokenizer(text1, text2,\n","                              return_attention_mask = True,\n","                              add_special_tokens = True,\n","                              padding = \"max_length\",\n","                              return_tensors = \"np\")\n","      \n","      ids = inputs[\"input_ids\"]\n","      token_type_ids = inputs[\"token_type_ids\"]\n","      mask = inputs[\"attention_mask\"]\n","      \n","      return {\n","              'ids': ids,\n","              'mask': mask,\n","              'token_type_ids': token_type_ids,\n","              'target': torch.tensor(self.df.iloc[index][\"label\"], dtype=torch.long)\n","              }\n","    if self.tk_with == \"xlm-roberta\":\n","      inputs = self.tokenizer(text1, text2,\n","                              return_attention_mask = True,\n","                              return_token_type_ids = True,\n","                              add_special_tokens = True,\n","                              padding = \"max_length\",\n","                              return_tensors = \"np\")\n","      \n","      ids = inputs[\"input_ids\"]\n","      mask = inputs[\"attention_mask\"]\n","      token_type_ids = inputs[\"token_type_ids\"]\n","      \n","      return {'ids': ids,\n","              'mask': mask,\n","              'token_type_ids': token_type_ids,\n","              'target': torch.tensor(self.df.iloc[index][\"label\"], dtype=torch.long)\n","              } \n","\n","# dataloader = DataLoader(dataset = dataset, batch_size = BATCH_SIZE)"],"metadata":{"id":"ymSHk-a0W_tu","executionInfo":{"status":"ok","timestamp":1680109497560,"user_tz":240,"elapsed":2,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",\n","                                               model_max_length = MODEL_MAX_LENGTH,\n","                                              padding_side = \"right\",\n","                                              truncation_side = \"right\",)"],"metadata":{"id":"RazX0NAMRqqG","executionInfo":{"status":"ok","timestamp":1680109941391,"user_tz":240,"elapsed":168,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["df_train = pd.read_csv(\"/content/drive/MyDrive/Personal Projects/NLP POCs/contradictory-my-dear-watson/train.csv\")\n","df_test = pd.read_csv(\"/content/drive/MyDrive/Personal Projects/NLP POCs/contradictory-my-dear-watson/test.csv\")"],"metadata":{"id":"Mev0DwLl4Kr1","executionInfo":{"status":"ok","timestamp":1680109537243,"user_tz":240,"elapsed":1568,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_val, y_train, y_val = train_test_split(df_train.index.values, \n","                                                  df_train.label.values, \n","                                                  test_size=0.15, \n","                                                  random_state=42, \n","                                                  stratify=df_train.label.values)\n","\n","df_train['data_type'] = \"\"\n","\n","df_train.loc[X_train, 'data_type'] = 'train'\n","df_train.loc[X_val, 'data_type'] = 'val'"],"metadata":{"id":"lcmHAwdHS2Ah","executionInfo":{"status":"ok","timestamp":1680109949475,"user_tz":240,"elapsed":169,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["def encode_one(x):\n","  encoded = bert_tokenizer(x['hypothesis'], x['premise'],\n","                           return_attention_mask = True,\n","                            add_special_tokens = True,\n","                            padding = \"max_length\",\n","                            truncation = 'longest_first',\n","                            return_tensors = \"np\") \n","  return encoded['input_ids'], encoded['token_type_ids'], encoded['attention_mask']"],"metadata":{"id":"XpiLEFD1Vdja","executionInfo":{"status":"ok","timestamp":1680110130705,"user_tz":240,"elapsed":154,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["df_train[[\"input_ids\",\"token_type_ids\", \"attention_mask\"]] = df_train.apply(lambda x : encode_one(x), axis = 1, result_type = 'expand')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F4ih8mztSmWp","executionInfo":{"status":"ok","timestamp":1680110151970,"user_tz":240,"elapsed":19816,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}},"outputId":"25aeb504-5830-4699-aec8-435b8c6eea2a"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stderr","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]}]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset"],"metadata":{"id":"tqmSmDmRShsF","executionInfo":{"status":"ok","timestamp":1680110221365,"user_tz":240,"elapsed":432,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["filter_dtype_train = df_train.data_type == \"train\"\n","\n","input_ids_train = torch.from_numpy(np.stack(df_train.loc[filter_dtype_train,'input_ids'].values))\n","attention_masks_train = torch.from_numpy(np.stack(df_train.loc[filter_dtype_train,'attention_mask'].values))\n","labels_train = torch.tensor(df_train.loc[filter_dtype_train,'label'].values)\n","\n","input_ids_val = torch.from_numpy(np.stack(df_train.loc[~filter_dtype_train,'input_ids'].values))\n","attention_masks_val = torch.from_numpy(np.stack(df_train.loc[~filter_dtype_train,'attention_mask'].values))\n","labels_val = torch.tensor(df_train.loc[~filter_dtype_train,'label'].values)\n","\n","dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n","dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"],"metadata":{"id":"BQ_MH87qSg_w","executionInfo":{"status":"ok","timestamp":1680110227279,"user_tz":240,"elapsed":431,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","dataloader_train = DataLoader(dataset_train, \n","                              sampler=SequentialSampler(dataset_train), \n","                              batch_size=BATCH_SIZE)\n","\n","dataloader_validation = DataLoader(dataset_val, \n","                                   sampler=SequentialSampler(dataset_val), \n","                                   batch_size=BATCH_SIZE)"],"metadata":{"id":"SKE3eZZ6SeHI","executionInfo":{"status":"ok","timestamp":1680110230134,"user_tz":240,"elapsed":146,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["df_train = df_train[df_train.lang_abv == \"en\"].reset_index(drop = True)\n","df_test = df_test[df_test.lang_abv == \"en\"].reset_index(drop = True)"],"metadata":{"id":"K25LTj3_4Y3A","executionInfo":{"status":"ok","timestamp":1680110232084,"user_tz":240,"elapsed":210,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["df_train.label.value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2QkoHTj84cQt","executionInfo":{"status":"ok","timestamp":1680110234007,"user_tz":240,"elapsed":6,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}},"outputId":"6fa533ce-9c0c-492e-b440-ef29a4c645a1"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    2427\n","2    2277\n","1    2166\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["epochs = 3"],"metadata":{"id":"mIeB5YBCXb1i","executionInfo":{"status":"ok","timestamp":1680110236172,"user_tz":240,"elapsed":167,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["class BERT(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.bert_model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n","        self.dp1 = nn.Dropout(p=0.1)\n","        self.ffnn1 = nn.Linear(768, 3)\n","        \n","        \n","    def forward(self,input_ids,attention_mask,token_type_ids, labels):\n","        _,o2= self.bert_model(input_ids = ids,\n","                              attention_mask = mask,\n","                              token_type_ids = token_type_ids, \n","                              return_dict=False)\n","        \n","        out = self.dp1(o2)\n","        out = torch.sigmoid((self.ffnn1(out)))\n","        \n","        return out\n","    \n","model=BERT()"],"metadata":{"id":"BVOf_qLAKPQq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680110493857,"user_tz":240,"elapsed":2811,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}},"outputId":"affd7cd7-9cc0-401e-f6ca-3f7aedeea7ac"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"metadata":{"id":"H2OBaf1bPxi4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680110283543,"user_tz":240,"elapsed":139,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}},"outputId":"3f95f690-25d1-45ad-cafa-786c97f3b1a5"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["# for param in model.xlm_roberta_model.parameters():\n","#     param.requires_grad = True"],"metadata":{"id":"6eA0E0KlLWg-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for p,p_x in model.named_parameters():\n","    if p_x.requires_grad:\n","      print(p)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SvqsL1HVLIDR","executionInfo":{"status":"ok","timestamp":1680110287533,"user_tz":240,"elapsed":839,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}},"outputId":"99e15fbe-52fd-4654-ed00-6f15383e39de"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["bert_model.embeddings.word_embeddings.weight\n","bert_model.embeddings.position_embeddings.weight\n","bert_model.embeddings.token_type_embeddings.weight\n","bert_model.embeddings.LayerNorm.weight\n","bert_model.embeddings.LayerNorm.bias\n","bert_model.encoder.layer.0.attention.self.query.weight\n","bert_model.encoder.layer.0.attention.self.query.bias\n","bert_model.encoder.layer.0.attention.self.key.weight\n","bert_model.encoder.layer.0.attention.self.key.bias\n","bert_model.encoder.layer.0.attention.self.value.weight\n","bert_model.encoder.layer.0.attention.self.value.bias\n","bert_model.encoder.layer.0.attention.output.dense.weight\n","bert_model.encoder.layer.0.attention.output.dense.bias\n","bert_model.encoder.layer.0.attention.output.LayerNorm.weight\n","bert_model.encoder.layer.0.attention.output.LayerNorm.bias\n","bert_model.encoder.layer.0.intermediate.dense.weight\n","bert_model.encoder.layer.0.intermediate.dense.bias\n","bert_model.encoder.layer.0.output.dense.weight\n","bert_model.encoder.layer.0.output.dense.bias\n","bert_model.encoder.layer.0.output.LayerNorm.weight\n","bert_model.encoder.layer.0.output.LayerNorm.bias\n","bert_model.encoder.layer.1.attention.self.query.weight\n","bert_model.encoder.layer.1.attention.self.query.bias\n","bert_model.encoder.layer.1.attention.self.key.weight\n","bert_model.encoder.layer.1.attention.self.key.bias\n","bert_model.encoder.layer.1.attention.self.value.weight\n","bert_model.encoder.layer.1.attention.self.value.bias\n","bert_model.encoder.layer.1.attention.output.dense.weight\n","bert_model.encoder.layer.1.attention.output.dense.bias\n","bert_model.encoder.layer.1.attention.output.LayerNorm.weight\n","bert_model.encoder.layer.1.attention.output.LayerNorm.bias\n","bert_model.encoder.layer.1.intermediate.dense.weight\n","bert_model.encoder.layer.1.intermediate.dense.bias\n","bert_model.encoder.layer.1.output.dense.weight\n","bert_model.encoder.layer.1.output.dense.bias\n","bert_model.encoder.layer.1.output.LayerNorm.weight\n","bert_model.encoder.layer.1.output.LayerNorm.bias\n","bert_model.encoder.layer.2.attention.self.query.weight\n","bert_model.encoder.layer.2.attention.self.query.bias\n","bert_model.encoder.layer.2.attention.self.key.weight\n","bert_model.encoder.layer.2.attention.self.key.bias\n","bert_model.encoder.layer.2.attention.self.value.weight\n","bert_model.encoder.layer.2.attention.self.value.bias\n","bert_model.encoder.layer.2.attention.output.dense.weight\n","bert_model.encoder.layer.2.attention.output.dense.bias\n","bert_model.encoder.layer.2.attention.output.LayerNorm.weight\n","bert_model.encoder.layer.2.attention.output.LayerNorm.bias\n","bert_model.encoder.layer.2.intermediate.dense.weight\n","bert_model.encoder.layer.2.intermediate.dense.bias\n","bert_model.encoder.layer.2.output.dense.weight\n","bert_model.encoder.layer.2.output.dense.bias\n","bert_model.encoder.layer.2.output.LayerNorm.weight\n","bert_model.encoder.layer.2.output.LayerNorm.bias\n","bert_model.encoder.layer.3.attention.self.query.weight\n","bert_model.encoder.layer.3.attention.self.query.bias\n","bert_model.encoder.layer.3.attention.self.key.weight\n","bert_model.encoder.layer.3.attention.self.key.bias\n","bert_model.encoder.layer.3.attention.self.value.weight\n","bert_model.encoder.layer.3.attention.self.value.bias\n","bert_model.encoder.layer.3.attention.output.dense.weight\n","bert_model.encoder.layer.3.attention.output.dense.bias\n","bert_model.encoder.layer.3.attention.output.LayerNorm.weight\n","bert_model.encoder.layer.3.attention.output.LayerNorm.bias\n","bert_model.encoder.layer.3.intermediate.dense.weight\n","bert_model.encoder.layer.3.intermediate.dense.bias\n","bert_model.encoder.layer.3.output.dense.weight\n","bert_model.encoder.layer.3.output.dense.bias\n","bert_model.encoder.layer.3.output.LayerNorm.weight\n","bert_model.encoder.layer.3.output.LayerNorm.bias\n","bert_model.encoder.layer.4.attention.self.query.weight\n","bert_model.encoder.layer.4.attention.self.query.bias\n","bert_model.encoder.layer.4.attention.self.key.weight\n","bert_model.encoder.layer.4.attention.self.key.bias\n","bert_model.encoder.layer.4.attention.self.value.weight\n","bert_model.encoder.layer.4.attention.self.value.bias\n","bert_model.encoder.layer.4.attention.output.dense.weight\n","bert_model.encoder.layer.4.attention.output.dense.bias\n","bert_model.encoder.layer.4.attention.output.LayerNorm.weight\n","bert_model.encoder.layer.4.attention.output.LayerNorm.bias\n","bert_model.encoder.layer.4.intermediate.dense.weight\n","bert_model.encoder.layer.4.intermediate.dense.bias\n","bert_model.encoder.layer.4.output.dense.weight\n","bert_model.encoder.layer.4.output.dense.bias\n","bert_model.encoder.layer.4.output.LayerNorm.weight\n","bert_model.encoder.layer.4.output.LayerNorm.bias\n","bert_model.encoder.layer.5.attention.self.query.weight\n","bert_model.encoder.layer.5.attention.self.query.bias\n","bert_model.encoder.layer.5.attention.self.key.weight\n","bert_model.encoder.layer.5.attention.self.key.bias\n","bert_model.encoder.layer.5.attention.self.value.weight\n","bert_model.encoder.layer.5.attention.self.value.bias\n","bert_model.encoder.layer.5.attention.output.dense.weight\n","bert_model.encoder.layer.5.attention.output.dense.bias\n","bert_model.encoder.layer.5.attention.output.LayerNorm.weight\n","bert_model.encoder.layer.5.attention.output.LayerNorm.bias\n","bert_model.encoder.layer.5.intermediate.dense.weight\n","bert_model.encoder.layer.5.intermediate.dense.bias\n","bert_model.encoder.layer.5.output.dense.weight\n","bert_model.encoder.layer.5.output.dense.bias\n","bert_model.encoder.layer.5.output.LayerNorm.weight\n","bert_model.encoder.layer.5.output.LayerNorm.bias\n","bert_model.encoder.layer.6.attention.self.query.weight\n","bert_model.encoder.layer.6.attention.self.query.bias\n","bert_model.encoder.layer.6.attention.self.key.weight\n","bert_model.encoder.layer.6.attention.self.key.bias\n","bert_model.encoder.layer.6.attention.self.value.weight\n","bert_model.encoder.layer.6.attention.self.value.bias\n","bert_model.encoder.layer.6.attention.output.dense.weight\n","bert_model.encoder.layer.6.attention.output.dense.bias\n","bert_model.encoder.layer.6.attention.output.LayerNorm.weight\n","bert_model.encoder.layer.6.attention.output.LayerNorm.bias\n","bert_model.encoder.layer.6.intermediate.dense.weight\n","bert_model.encoder.layer.6.intermediate.dense.bias\n","bert_model.encoder.layer.6.output.dense.weight\n","bert_model.encoder.layer.6.output.dense.bias\n","bert_model.encoder.layer.6.output.LayerNorm.weight\n","bert_model.encoder.layer.6.output.LayerNorm.bias\n","bert_model.encoder.layer.7.attention.self.query.weight\n","bert_model.encoder.layer.7.attention.self.query.bias\n","bert_model.encoder.layer.7.attention.self.key.weight\n","bert_model.encoder.layer.7.attention.self.key.bias\n","bert_model.encoder.layer.7.attention.self.value.weight\n","bert_model.encoder.layer.7.attention.self.value.bias\n","bert_model.encoder.layer.7.attention.output.dense.weight\n","bert_model.encoder.layer.7.attention.output.dense.bias\n","bert_model.encoder.layer.7.attention.output.LayerNorm.weight\n","bert_model.encoder.layer.7.attention.output.LayerNorm.bias\n","bert_model.encoder.layer.7.intermediate.dense.weight\n","bert_model.encoder.layer.7.intermediate.dense.bias\n","bert_model.encoder.layer.7.output.dense.weight\n","bert_model.encoder.layer.7.output.dense.bias\n","bert_model.encoder.layer.7.output.LayerNorm.weight\n","bert_model.encoder.layer.7.output.LayerNorm.bias\n","bert_model.encoder.layer.8.attention.self.query.weight\n","bert_model.encoder.layer.8.attention.self.query.bias\n","bert_model.encoder.layer.8.attention.self.key.weight\n","bert_model.encoder.layer.8.attention.self.key.bias\n","bert_model.encoder.layer.8.attention.self.value.weight\n","bert_model.encoder.layer.8.attention.self.value.bias\n","bert_model.encoder.layer.8.attention.output.dense.weight\n","bert_model.encoder.layer.8.attention.output.dense.bias\n","bert_model.encoder.layer.8.attention.output.LayerNorm.weight\n","bert_model.encoder.layer.8.attention.output.LayerNorm.bias\n","bert_model.encoder.layer.8.intermediate.dense.weight\n","bert_model.encoder.layer.8.intermediate.dense.bias\n","bert_model.encoder.layer.8.output.dense.weight\n","bert_model.encoder.layer.8.output.dense.bias\n","bert_model.encoder.layer.8.output.LayerNorm.weight\n","bert_model.encoder.layer.8.output.LayerNorm.bias\n","bert_model.encoder.layer.9.attention.self.query.weight\n","bert_model.encoder.layer.9.attention.self.query.bias\n","bert_model.encoder.layer.9.attention.self.key.weight\n","bert_model.encoder.layer.9.attention.self.key.bias\n","bert_model.encoder.layer.9.attention.self.value.weight\n","bert_model.encoder.layer.9.attention.self.value.bias\n","bert_model.encoder.layer.9.attention.output.dense.weight\n","bert_model.encoder.layer.9.attention.output.dense.bias\n","bert_model.encoder.layer.9.attention.output.LayerNorm.weight\n","bert_model.encoder.layer.9.attention.output.LayerNorm.bias\n","bert_model.encoder.layer.9.intermediate.dense.weight\n","bert_model.encoder.layer.9.intermediate.dense.bias\n","bert_model.encoder.layer.9.output.dense.weight\n","bert_model.encoder.layer.9.output.dense.bias\n","bert_model.encoder.layer.9.output.LayerNorm.weight\n","bert_model.encoder.layer.9.output.LayerNorm.bias\n","bert_model.encoder.layer.10.attention.self.query.weight\n","bert_model.encoder.layer.10.attention.self.query.bias\n","bert_model.encoder.layer.10.attention.self.key.weight\n","bert_model.encoder.layer.10.attention.self.key.bias\n","bert_model.encoder.layer.10.attention.self.value.weight\n","bert_model.encoder.layer.10.attention.self.value.bias\n","bert_model.encoder.layer.10.attention.output.dense.weight\n","bert_model.encoder.layer.10.attention.output.dense.bias\n","bert_model.encoder.layer.10.attention.output.LayerNorm.weight\n","bert_model.encoder.layer.10.attention.output.LayerNorm.bias\n","bert_model.encoder.layer.10.intermediate.dense.weight\n","bert_model.encoder.layer.10.intermediate.dense.bias\n","bert_model.encoder.layer.10.output.dense.weight\n","bert_model.encoder.layer.10.output.dense.bias\n","bert_model.encoder.layer.10.output.LayerNorm.weight\n","bert_model.encoder.layer.10.output.LayerNorm.bias\n","bert_model.encoder.layer.11.attention.self.query.weight\n","bert_model.encoder.layer.11.attention.self.query.bias\n","bert_model.encoder.layer.11.attention.self.key.weight\n","bert_model.encoder.layer.11.attention.self.key.bias\n","bert_model.encoder.layer.11.attention.self.value.weight\n","bert_model.encoder.layer.11.attention.self.value.bias\n","bert_model.encoder.layer.11.attention.output.dense.weight\n","bert_model.encoder.layer.11.attention.output.dense.bias\n","bert_model.encoder.layer.11.attention.output.LayerNorm.weight\n","bert_model.encoder.layer.11.attention.output.LayerNorm.bias\n","bert_model.encoder.layer.11.intermediate.dense.weight\n","bert_model.encoder.layer.11.intermediate.dense.bias\n","bert_model.encoder.layer.11.output.dense.weight\n","bert_model.encoder.layer.11.output.dense.bias\n","bert_model.encoder.layer.11.output.LayerNorm.weight\n","bert_model.encoder.layer.11.output.LayerNorm.bias\n","bert_model.pooler.dense.weight\n","bert_model.pooler.dense.bias\n","ffnn1.weight\n","ffnn1.bias\n"]}]},{"cell_type":"code","source":["SAVE_DIR = \"/content/drive/MyDrive/Personal Projects/NLP POCs/contradictory-my-dear-watson/models\"\n","MODEL_NAME = 'BERT_model_v2.0.pth'"],"metadata":{"id":"uFaxOjoleRTS","executionInfo":{"status":"ok","timestamp":1680110291743,"user_tz":240,"elapsed":155,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["# path = os.path.join(SAVE_DIR, MODEL_NAME)\n","# model = BERT()\n","# model = model.load_state_dict(torch.load(path))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2eLCx45AHbnV","executionInfo":{"status":"ok","timestamp":1680106104543,"user_tz":240,"elapsed":6042,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}},"outputId":"ec709b1c-910f-45ed-f49f-00abea1e7e20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KdINpMQyHsUZ","executionInfo":{"status":"ok","timestamp":1680110501948,"user_tz":240,"elapsed":443,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}},"outputId":"ed228e55-d781-42ea-eb1a-86058c5c7af1"},"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BERT(\n","  (bert_model): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dp1): Dropout(p=0.1, inplace=False)\n","  (ffnn1): Linear(in_features=768, out_features=3, bias=True)\n",")"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["optimizer= AdamW(model.parameters(), lr=1e-4)\n","\n","seed_val = 17\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","def evaluate(dataloader_val):\n","\n","    model.eval()\n","    \n","    loss_val_total = 0\n","    predictions, true_vals = [], []\n","    \n","    for batch in dataloader_val:\n","        \n","        batch = tuple(b.to(device) for b in batch)\n","        \n","        inputs = {'input_ids':      batch[0].view(-1,MODEL_MAX_LENGTH),\n","                  'attention_mask': batch[1].view(-1,MODEL_MAX_LENGTH),\n","                  'labels':         batch[2]\n","                 }     \n","        with torch.no_grad():        \n","            outputs = model(**inputs)\n","            \n","        loss = outputs[0]\n","        logits = outputs[1]\n","        loss_val_total += loss.item()\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = inputs['labels'].cpu().numpy()\n","        predictions.append(logits)\n","        true_vals.append(label_ids)\n","    \n","    loss_val_avg = loss_val_total/len(dataloader_val) \n","    \n","    predictions = np.concatenate(predictions, axis=0)\n","    true_vals = np.concatenate(true_vals, axis=0)\n","            \n","    return loss_val_avg, predictions, true_vals"],"metadata":{"id":"ME4nLVgqKO9p","executionInfo":{"status":"ok","timestamp":1680110407115,"user_tz":240,"elapsed":175,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import f1_score\n","\n","def f1_score_func(preds, labels):\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return f1_score(labels_flat, preds_flat, average='weighted')"],"metadata":{"id":"DwNy3tu4KO4L","executionInfo":{"status":"ok","timestamp":1680110326820,"user_tz":240,"elapsed":195,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["for epoch in tqdm(range(1, epochs+1)):\n","    \n","    model.train()\n","    \n","    loss_train_total = 0\n","\n","    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n","    for batch in progress_bar:\n","\n","        model.zero_grad()\n","        \n","        batch = tuple(b.to(device) for b in batch)\n","        \n","        inputs = {'input_ids':      batch[0].view(-1,MODEL_MAX_LENGTH),\n","                  'attention_mask': batch[1].view(-1,MODEL_MAX_LENGTH),\n","                  \"token_type_ids\" : None,\n","                  'labels':         batch[2]\n","                 }      \n","\n","        outputs = model(**inputs)\n","        \n","        loss = F.cross_entropy(inputs['labels'], outputs['logits'].softmax(dim=1))\n","        # loss = outputs[0]\n","        loss_train_total += loss.item()\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        optimizer.step()\n","        # scheduler.step()\n","        \n","        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n","         \n","        \n","    # torch.save(model.state_dict(), f'data_volume/finetuned_BERT_epoch_{epoch}.model')\n","        \n","    tqdm.write(f'\\nEpoch {epoch}')\n","    \n","    loss_train_avg = loss_train_total/len(dataloader_train)            \n","    tqdm.write(f'Training loss: {loss_train_avg}')\n","    \n","    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n","    val_f1 = f1_score_func(predictions, true_vals)\n","    tqdm.write(f'Validation loss: {val_loss}')\n","    tqdm.write(f'F1 Score (Weighted): {val_f1}')"],"metadata":{"id":"InRna6OHKOzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","    \n","loss_val_total = 0\n","predictions, true_vals = [], []\n","\n","for batch in dataloader_validation:\n","    \n","    batch = tuple(b.to(device) for b in batch)\n","    \n","    inputs = {'input_ids':      batch[0].view(-1,MODEL_MAX_LENGTH),\n","              'attention_mask': batch[1].view(-1,MODEL_MAX_LENGTH),\n","              'labels':         batch[2]\n","              }     \n","    with torch.no_grad():        \n","        outputs = model(**inputs)\n","        \n","    loss = outputs[0]\n","    logits = outputs[1]\n","    loss_val_total += loss.item()\n","\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = inputs['labels'].cpu().numpy()\n","    predictions.append(logits)\n","    true_vals.append(label_ids)\n","\n","loss_val_avg = loss_val_total/len(dataloader_validation) \n","\n","predictions = np.concatenate(predictions, axis=0)\n","true_vals = np.concatenate(true_vals, axis=0)"],"metadata":{"id":"aeUmacCxKOuy","colab":{"base_uri":"https://localhost:8080/","height":166},"executionInfo":{"status":"error","timestamp":1680107370677,"user_tz":240,"elapsed":415,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}},"outputId":"58cd23c6-9991-483b-a50b-e662aa1654ac"},"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-db6fc64dcbe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrue_vals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_vals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: 'bool' object is not iterable"]}]},{"cell_type":"code","source":["sum(predictions.argmax(axis=1) == true_vals) / len(true_vals) * 100"],"metadata":{"id":"cTMiNEmhKOrw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680107406738,"user_tz":240,"elapsed":6,"user":{"displayName":"Aadil Iliyas Zikre","userId":"11139065713760395341"}},"outputId":"f444d560-6c63-466f-91a6-b2c1303d845a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{2}"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":[],"metadata":{"id":"AzFCVodCKOnY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zWGEDa9DKOkX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VZBedfCvKOTA"},"execution_count":null,"outputs":[]}]}